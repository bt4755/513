{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bt4755/513/blob/main/513Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxbtUS4TishM",
        "outputId": "159a4711-1f8e-4957-c2ca-4299c19d3b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Collecting edward\n",
            "  Downloading edward-1.3.5.tar.gz (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from edward) (1.21.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from edward) (1.15.0)\n",
            "Building wheels for collected packages: edward\n",
            "  Building wheel for edward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for edward: filename=edward-1.3.5-py3-none-any.whl size=90385 sha256=722f862825fb548d0b0c8dba75e8476e5729595a8586e77186de946ae59d1543\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/d0/6f/298c809cc8db6573b8642d1473c668868a67b9ad297c2e8b65\n",
            "Successfully built edward\n",
            "Installing collected packages: edward\n",
            "Successfully installed edward-1.3.5\n",
            "Collecting torchbnn\n",
            "  Downloading torchbnn-1.2-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: torchbnn\n",
            "Successfully installed torchbnn-1.2\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.24.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 27.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (13.0.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "Installing collected packages: tf-estimator-nightly\n",
            "Successfully installed tf-estimator-nightly-2.8.0.dev2021122109\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!pip install edward\n",
        "!pip install torchbnn\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xd8Tzvql2ZAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85005625-6cff-44df-ce77-6c91a42a0e03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%rm -rf \"/content/drive/My Drive/513/src/logs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsBLW2Ekig8W",
        "outputId": "3580d399-bbae-4ecd-d267-8e8eeb4f8687"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/513/src\n"
          ]
        }
      ],
      "source": [
        "%cd \"/content/drive/My Drive/513/src/\"\n",
        "import os\n",
        "!CUDA_LAUNCH_BLOCKING=1\n",
        "os.system('CUDA_LAUNCH_BLOCKING=1')\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import cuda\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch._C import NoneType\n",
        "import torchbnn as bnn\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import timeit\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import math\n",
        "import argparse\n",
        "import torch\n",
        "from torchvision.transforms.transforms import Resize, Compose, ToPILImage, ToTensor, Normalize\n",
        "import torch.utils.data as data\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "from skimage import io\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "II-0avdR-qDB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "5OSxhQm-psg6"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.disc = nn.Sequential(\n",
        "            # input: N x channels_img x 64 x 64\n",
        "            nn.Conv2d(\n",
        "                channels_img, features_d, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
        "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
        "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
        "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
        "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
        "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
        "            #nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.LeakyReLU(0.2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.disc(x)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g):\n",
        "        super(Generator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # Input: N x channels_noise x 1 x 1\n",
        "            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
        "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
        "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
        "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            # Output: N x channels_img x 64 x 64\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(\n",
        "                in_channels,\n",
        "                out_channels,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                bias=False,\n",
        "            ),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def initialize_weights(model):\n",
        "    # Initializes weights according to the DCGAN paper\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
        "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "class pokemonDataset(Dataset):\n",
        "  def __init__(self, root_dir, size, transform):\n",
        "      self.root_dir = root_dir\n",
        "      self.transform = transform\n",
        "      self.size = size\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      img = io.imread(\"{root}/{idx}.png\".format(root= self.root_dir, idx = index + 1))\n",
        "      img = self.transform(img)\n",
        "      return img, 0\n",
        "\n",
        "  def __len__(self):\n",
        "      return self.size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UXLjSH4qc5y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9291016d-6b42-4bd6-f1d9-5832d74d249a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20] Batch 0/469                   Loss D: 0.6938, loss G: 0.7155\n",
            "Epoch [1/20] Batch 100/469                   Loss D: 0.1141, loss G: 2.1690\n",
            "Epoch [1/20] Batch 200/469                   Loss D: 0.0270, loss G: 3.5435\n",
            "Epoch [1/20] Batch 300/469                   Loss D: 0.4503, loss G: 1.2282\n",
            "Epoch [1/20] Batch 400/469                   Loss D: 0.6299, loss G: 0.8744\n",
            "Epoch [2/20] Batch 0/469                   Loss D: 0.8492, loss G: 0.2591\n",
            "Epoch [2/20] Batch 100/469                   Loss D: 0.6200, loss G: 0.7189\n",
            "Epoch [2/20] Batch 200/469                   Loss D: 0.6170, loss G: 0.9306\n",
            "Epoch [2/20] Batch 300/469                   Loss D: 0.6372, loss G: 0.8651\n",
            "Epoch [2/20] Batch 400/469                   Loss D: 0.6534, loss G: 0.7642\n",
            "Epoch [3/20] Batch 0/469                   Loss D: 0.6567, loss G: 0.6581\n",
            "Epoch [3/20] Batch 100/469                   Loss D: 0.6730, loss G: 0.9352\n",
            "Epoch [3/20] Batch 200/469                   Loss D: 0.6562, loss G: 0.7387\n",
            "Epoch [3/20] Batch 300/469                   Loss D: 0.6831, loss G: 0.8548\n",
            "Epoch [3/20] Batch 400/469                   Loss D: 0.6517, loss G: 0.7862\n",
            "Epoch [4/20] Batch 0/469                   Loss D: 0.6426, loss G: 0.8053\n",
            "Epoch [4/20] Batch 100/469                   Loss D: 0.6486, loss G: 0.8019\n",
            "Epoch [4/20] Batch 200/469                   Loss D: 0.6480, loss G: 0.8321\n",
            "Epoch [4/20] Batch 300/469                   Loss D: 0.6682, loss G: 0.6085\n",
            "Epoch [4/20] Batch 400/469                   Loss D: 0.6414, loss G: 0.6585\n",
            "Epoch [5/20] Batch 0/469                   Loss D: 0.6386, loss G: 0.9133\n",
            "Epoch [5/20] Batch 100/469                   Loss D: 0.6358, loss G: 0.7073\n",
            "Epoch [5/20] Batch 200/469                   Loss D: 0.6271, loss G: 0.8173\n",
            "Epoch [5/20] Batch 300/469                   Loss D: 0.6210, loss G: 0.8505\n",
            "Epoch [5/20] Batch 400/469                   Loss D: 0.6266, loss G: 0.5281\n",
            "Epoch [6/20] Batch 0/469                   Loss D: 0.6032, loss G: 0.8113\n",
            "Epoch [6/20] Batch 100/469                   Loss D: 0.6260, loss G: 0.9648\n",
            "Epoch [6/20] Batch 200/469                   Loss D: 0.6154, loss G: 0.7175\n",
            "Epoch [6/20] Batch 300/469                   Loss D: 0.6117, loss G: 0.9224\n",
            "Epoch [6/20] Batch 400/469                   Loss D: 0.6509, loss G: 1.0596\n",
            "Epoch [7/20] Batch 0/469                   Loss D: 0.5866, loss G: 0.6899\n",
            "Epoch [7/20] Batch 100/469                   Loss D: 0.5783, loss G: 0.9371\n",
            "Epoch [7/20] Batch 200/469                   Loss D: 0.5685, loss G: 0.9553\n",
            "Epoch [7/20] Batch 300/469                   Loss D: 0.7273, loss G: 0.9978\n",
            "Epoch [7/20] Batch 400/469                   Loss D: 0.4956, loss G: 1.0933\n",
            "Epoch [8/20] Batch 0/469                   Loss D: 0.5765, loss G: 0.9440\n",
            "Epoch [8/20] Batch 100/469                   Loss D: 0.5913, loss G: 1.9702\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters etc.\n",
        "D_SET = 'MNIST'\n",
        "#D_SET ='POKE'\n",
        "LEARNING_RATE = 3e-4  # could also use two lrs, one for gen and one for disc\n",
        "BATCH_SIZE = 128\n",
        "IMG_SIZE = 64\n",
        "IMG_CHANNELS = 0\n",
        "if D_SET == 'MNIST':\n",
        "  IMG_CHANNELS = 1\n",
        "elif D_SET == 'POKE':\n",
        "  IMG_CHANNELS = 4\n",
        "else:\n",
        "  IMG_CHANNELS = 3\n",
        "LEARNING_RATE = 2e-4\n",
        "NOISE_DIM = 100\n",
        "NUM_EPOCHS = 20\n",
        "feature_size = 16\n",
        "DATA_PATH = None\n",
        "if D_SET == 'MNIST':\n",
        "  DATA_PATH = \"../data\"\n",
        "elif D_SET == 'POKE':\n",
        "  DATA_PATH = \"Datasets/pokemon/pokemon/\"\n",
        "else:\n",
        "  DATA_PATH = None\n",
        "\n",
        "  \n",
        "torch.cuda.manual_seed(1)\n",
        "\n",
        "transformer = None\n",
        "dataset = None\n",
        "if D_SET == 'MNIST':\n",
        "  transformer = Compose(\n",
        "    [\n",
        "      Resize(IMG_SIZE),\n",
        "      ToTensor(),\n",
        "      Normalize(\n",
        "        [0.5 for _ in range(IMG_CHANNELS)], [0.5 for _ in range(IMG_CHANNELS)]\n",
        "      )\n",
        "    ]\n",
        "  )\n",
        "\n",
        "  dataset = datasets.MNIST(root='../data', train=True, download=True, transform=transformer)\n",
        "elif D_SET == 'POKE':\n",
        "  transformer = Compose(\n",
        "    [\n",
        "      ToPILImage(),\n",
        "      Resize(IMG_SIZE),\n",
        "      ToTensor(),\n",
        "      Normalize(\n",
        "        [0.5 for _ in range(IMG_CHANNELS)], [0.5 for _ in range(IMG_CHANNELS)]\n",
        "      )\n",
        "    ]\n",
        "  )\n",
        "  dataset = pokemonDataset(DATA_PATH,721, transformer)\n",
        "else:\n",
        "  pass\n",
        "\n",
        "\n",
        "# comment mnist above and uncomment below if train on CelebA\n",
        "#dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "gen = Generator(NOISE_DIM, IMG_CHANNELS, feature_size).to(device)\n",
        "disc = Discriminator(IMG_CHANNELS, feature_size).to(device)\n",
        "initialize_weights(gen)\n",
        "initialize_weights(disc)\n",
        "\n",
        "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "#criterion = nn.BCELoss()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "fixed_noise = torch.randn((BATCH_SIZE, NOISE_DIM, 1,1)).to(device)\n",
        "writer_real = SummaryWriter('logs/{}/Real'.format(D_SET))\n",
        "writer_fake = SummaryWriter('logs/{}/Fake'.format(D_SET))\n",
        "step = 0\n",
        "\n",
        "gen.train()\n",
        "disc.train()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Target labels not needed! <3 unsupervised\n",
        "    for batch_idx, (real, _) in enumerate(dataloader):\n",
        "        real = real.to(device)\n",
        "        noise = torch.randn(BATCH_SIZE, NOISE_DIM, 1, 1).to(device)\n",
        "        fake = gen(noise)\n",
        "\n",
        "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z)))\n",
        "        disc_real = disc(real).reshape(-1)\n",
        "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
        "        disc_fake = disc(fake.detach()).reshape(-1)\n",
        "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
        "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
        "        disc.zero_grad()\n",
        "        loss_disc.backward()\n",
        "        opt_disc.step()\n",
        "\n",
        "        ### Train Generator: min log(1 - D(G(z))) <-> max log(D(G(z))\n",
        "        output = disc(fake).reshape(-1)\n",
        "        loss_gen = criterion(output, torch.ones_like(output))\n",
        "        gen.zero_grad()\n",
        "        loss_gen.backward()\n",
        "        opt_gen.step()\n",
        "\n",
        "        # Print losses occasionally and print to tensorboard\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] Batch {batch_idx}/{len(dataloader)} \\\n",
        "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\"\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                fake = gen(fixed_noise)\n",
        "                # take out (up to) 32 examples\n",
        "                img_grid_real = torchvision.utils.make_grid(\n",
        "                    real[:32], normalize=True\n",
        "                )\n",
        "                img_grid_fake = torchvision.utils.make_grid(\n",
        "                    fake[:32], normalize=True\n",
        "                )\n",
        "\n",
        "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
        "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
        "\n",
        "            step += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rXxByQgqlem"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrLHWDZclFCY"
      },
      "outputs": [],
      "source": [
        "#from google.colab import files\n",
        "#files.upload()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "513Implementation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPworzRwY/Od6Nudq6Fh9zT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}